# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hIFUenJtS4TglCrDbVOz-GBa-nu_TgeH

##                            WQD7001 Principles of Data Science

##                                   Semester 2 /2022/2023

##                       

##                            E-Commerce Sales Prediction Using R

##                               Group Assignment (Part 1)

###                               GROUP 10 - Data Infinity

### Team Members:-

### Puvaneswari Poobalan (S2182524)

### Chong Hun Yee (S2197999)

### Ooi Mei Ling (22064004)

### Wang Chunli (22064827)

### Guan Meng (S2179731)

### 

## 1. Team Members & Roles

-   Puvaneswari Poobalan - Leader (<https://sites.google.com/view/s2182524?usp=sharing>)
-   Chong Hun Yee - Data Scientist (<https://sites.google.com/view/chong-hun-yee>)
-   Ool Mei Ling - Secretary (<https://sites.google.com/view/22064004?usp=sharing>)
-   Wang Chunli - Machine Learning Engineer (<https://sites.google.com/view/wangchunli?usp=sharing>)
-   Meng Guan - Information Designer (<https://sites.google.com/view/eportfolio-s2179731?usp=sharing>)

### 

## 2. Project Abstract

The goal of this project is to create a prediction model for e-commerce sales. By using Olist's Brazilian E-Commerce Public Dataset, which comprises information about orders, items, and consumers from January 2017 to August 2018. Besides that, the dataset also contains information about order progress, payment methods, and customer feedback. The primary focus is on the e-commerce market, where sales forecasting is critical for organizations looking to optimize their inventory and pricing strategies. The dataset allows for the prediction of future sales based on purchase date information, as well as the analysis of delivery performance and the optimization of delivery timings. Predictive analysis will be used in this project to forecast future e-commerce sales. A machine learning model will be contructed in order to predict future sales, discover patterns, and assist businesses in making educated decisions by analyzing the previous data.

### 

## 3. Introduction

Brazil's e-commerce sector has grown fast in recent years where the Brazilian e-commerce market expected to reach \$56.6 billion by the year of 2023. E-commerce has altered the way consumers shop. Hence, it is crucial to understand and leverage e-commerce data for business growth. Olist's Brazilian E-Commerce Public Dataset is a fantastic resource for researching e-commerce data, and it offers a wonderful opportunity to predict future sales using machine learning techniques. By predicting future sales trends accurately, it can help companies to develop more effective business strategies and decisions. With this, it will allow the companies to be able to adapt the changes in the market resulting in obtaining optimal sales performance.

R programming language will be used as the main tool in this project. A machine learning model which is capable of predicting e-commerce sales will be developed by using the Olist Brazilian e-commerce public dataset.

The process from acquiring and cleaning the dataset to exploratory data analysis (EDA) as well as developing the predictive model will be described clearly in this project. Finally, the findings of this project will be summarized. Throughout this project, it is hoped that more accurate and effective sales forecasting methods for e-commerce companies will be provided. In addition, more future researchers will be to inspired in studying related fields.

### 

## 4. Problem Statement

i.  Predicting sales for e-commerce enterprises is difficult. This is because it will affect their capacity to plan inventory, marketing campaigns, and other elements of their operations.
ii. Traditional methods of sales forecasting may be ineffective in the e-commerce industry, which are characterized by quick changes in customer behavior and market trends.
iii. Inaccurate sales forecasting will lead e-commerce enterprises to make wrong decisions. Hence, it is crucial to have an accurate sales prediction in order to make informed decisions and maximize income.

### 

## 5. Project Objective

i.  To develop a machine learning model using R which predicts e-commerce sales accurately based on historical data which consists of customer behavior, product attributes, and market trends.
ii. To evaluate the performance of the sales prediction model and identify area for improvement by using data preprocessing or feature engineering.
iii. To apply the sales prediction model to develop a framework for inventory planning, marketing campaign optimization, and other operational decisions which enable e-commerce businesses to maximize revenue and improve profitability.

### 

## 6. Scope & Domain

The scope of this project is to develop a machine learning model which will be capable of predicting e-commerce sales using the Olist Brazil e-commerce public dataset. The dataset covers order, product and consumer information from January 2017 to August 2018. This project's domain is e-commerce where it focuses on forecasting future sales.

### 

## 7. Summarize Literature Review

In the previous years, there are many researchers have made significant advancements in the field of e-commerce sales forecasting. In this section, different forecasting methods have been proposed and applied to the practical problems. The following is a summary of the literature review:

Many studies have been using machine learning methods for e-commerce sales forecasting. Zhu (2021) proposed an Extreme Learning Machine (ELM) based on forecasting method where the Whale Optimisation Algorithm (WOA) was used to optimize the forecasting. The method has shown better performance in dealing with non-linear relationships and multivariate data.

In recent years, deep learning methods have achieved remarkable results on prediction tasks. Huo (2021) explored the application of convolutional neural networks (CNNs) in e-commerce sales forecasting. Huo (2021) proposed a CNN-based multi-scale prediction model which improved prediction accuracy by learning both local and global features.

### 7.1 Feature Selection and Pre-processing

In order to improve the performance of prediction models, feature selection and pre-processing play a key role in e-commerce sales forecasting. Wei, Geng, Ying, and Li (2014) explored this feature selection and pre-processing can help to improve prediction accuracy in time series-based approaches to sales forecasting. The authors proposed a time series-based forecasting method and used dynamic time regularization (DTW) to process time series data in order to enhance the forecasting results.

### 7.2 Algorithm Optimization

Many studies have attempted to optimize existing methods to improve the performance of forecasting models. For instance, the Whale Optimisation Algorithm (WOA) was used to optimize the parameters of an Extreme Learning Machine (ELM), thereby improving the accuracy and robustness of the model (Zhu, 2021).

In a nutshell, reading the relevant literature review provides insights into the progress of research. Furthermore, this section highlights the main approaches in the field of e-commerce sales forecasting. In this study, these approaches will be combined to provide a more effective framework and techniques for e-commerce sales forecasting.

### 

## 8. DS Pathway

Our project will follow the DS Life Cycle, beginning with acquiring and cleaning the dataset and progressing to exploratory data analysis (EDA) and the development of a predictive model. The steps we shall take are as follows:

### 8.1 Data Set (Obtain & Clean)

The Brazilian E-Commerce Public Dataset by Olist from Kaggle was used in this project. The dataset provides information on 100k orders placed at several Brazilian marketplaces between 2016 and 2018. The dataset will be cleaned by removing duplicate records, deleting missing values, and translating variables into appropriate representations.

### 8.2 Collaborator/ End Users (Asking Questions)

This project's end users are Brazilian e-commerce companies. We will ask end-users the following questions:-

```         
i. What variables drive sales in your business?
ii. What are the best sale timings for your customers?
iii. What are the difficulties you have in forecasting future sales?
"""

# Commented out IPython magic to ensure Python compatibility.
#Data loading

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#Read files
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

customers_df= pd.read_csv('olist_customers_dataset.csv')
geolocation_df= pd.read_csv('olist_geolocation_dataset.csv')
items_df= pd.read_csv('olist_order_items_dataset.csv')
payments_df= pd.read_csv('olist_order_payments_dataset.csv')
reviews_df= pd.read_csv('olist_order_reviews_dataset.csv')
orders_df= pd.read_csv('olist_orders_dataset.csv')
products_df= pd.read_csv('olist_products_dataset.csv')
sellers_df= pd.read_csv('olist_sellers_dataset.csv')
category_translation_df= pd.read_csv('product_category_name_translation.csv')

#Data cleaning
#Merging all data

df= pd.merge(customers_df, orders_df, on="customer_id", how='inner')
df= df.merge(reviews_df, on="order_id", how='inner')
df= df.merge(items_df, on="order_id", how='inner')
df= df.merge(products_df, on="product_id", how='inner')
df= df.merge(payments_df, on="order_id", how='inner')
df= df.merge(sellers_df, on='seller_id', how='inner')
df= df.merge(category_translation_df, on='product_category_name', how='inner')
df.shape

#Show all features
df.columns

#Check duplicates
df.duplicated().sum()

df.describe()

df.info()

#Handling missing values

df.isna().sum()[:20]

#Drop all missing values in datetime columns
df.dropna(subset= ['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date'], inplace=True)

# Number of missing values for the vecond half of features

df.isna().sum()[20:]

#Check the missing values
df[['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']][df.product_weight_g.isna()]

# Since all the missing values are in the same raw, we will drop this raw
df.drop(27352, inplace=True)

# Reset Index
df.reset_index(inplace= True, drop= True)

"""Feature engineering"""

#Classify products categories (71) into 9 main categories

def classify_cat(x):

    if x in ['office_furniture', 'furniture_decor', 'furniture_living_room', 'kitchen_dining_laundry_garden_furniture', 'bed_bath_table', 'home_comfort', 'home_comfort_2', 'home_construction', 'garden_tools', 'furniture_bedroom', 'furniture_mattress_and_upholstery']:
        return 'Furniture'
    
    elif x in ['auto', 'computers_accessories', 'musical_instruments', 'consoles_games', 'watches_gifts', 'air_conditioning', 'telephony', 'electronics', 'fixed_telephony', 'tablets_printing_image', 'computers', 'small_appliances_home_oven_and_coffee', 'small_appliances', 'audio', 'signaling_and_security', 'security_and_services']:
        return 'Electronics'
    
    elif x in ['fashio_female_clothing', 'fashion_male_clothing', 'fashion_bags_accessories', 'fashion_shoes', 'fashion_sport', 'fashion_underwear_beach', 'fashion_childrens_clothes', 'baby', 'cool_stuff', ]:
        return 'Fashion'
        
    elif x in ['housewares', 'home_confort', 'home_appliances', 'home_appliances_2', 'flowers', 'costruction_tools_garden', 'garden_tools', 'construction_tools_lights', 'costruction_tools_tools', 'luggage_accessories', 'la_cuisine', 'pet_shop', 'market_place']:
        return 'Home & Garden'
        
    elif x in ['sports_leisure', 'toys', 'cds_dvds_musicals', 'music', 'dvds_blu_ray', 'cine_photo', 'party_supplies', 'christmas_supplies', 'arts_and_craftmanship', 'art']:
        return 'Entertainment'
    
    elif x in ['health_beauty', 'perfumery', 'diapers_and_hygiene']:
        return 'Beauty & Health'
    
    elif x in ['food_drink', 'drinks', 'food']:
        return 'Food & Drinks'
    
    elif x in ['books_general_interest', 'books_technical', 'books_imported', 'stationery']:
        return 'Books & Stationery'
    
    elif x in ['construction_tools_construction', 'construction_tools_safety', 'industry_commerce_and_business', 'agro_industry_and_commerce']:
        return 'Industry & Construction'

df['product_category'] = df.product_category_name_english.apply(classify_cat)

df.product_category.value_counts()

# Create Volume Column
df['product_vol_cm3'] = df.product_length_cm * df.product_width_cm * df.product_height_cm

# Drop Width, Height and Length
df.drop(['product_length_cm', 'product_width_cm', 'product_height_cm'], axis= 1, inplace= True)

#Convert datetime features from object to datetime

df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])
df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])
df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])
df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])
df['order_delivered_carrier_date'] =pd.to_datetime(df['order_delivered_carrier_date'])

#Extract duration of estimated shipping from purchasing date untill estimated delivery date
df['estimated_days'] = (df['order_estimated_delivery_date'].dt.date - df['order_purchase_timestamp'].dt.date).dt.days

#Extract duration of shipping from purchasing date until delivered to customer date
df['arrival_days'] = (df['order_delivered_customer_date'].dt.date - df['order_purchase_timestamp'].dt.date).dt.days

#Extract duration of shipping from purchasing carrier delivered date untill delivered to customer
df['shipping_days'] = (df['order_delivered_customer_date'].dt.date - df['order_delivered_carrier_date'].dt.date).dt.days

#Drop inconsistent dates where "order_delivered_carrier_date" is greater than "order_delivered_customer_date"
df.drop((df[['order_delivered_carrier_date', 'order_delivered_customer_date']][df.shipping_days < 0]).index, inplace= True)

#Shipping status from Seller to Carrier

# First get seller to carrier duration in days
df['seller_to_carrier_status'] = (df['shipping_limit_date'].dt.date - df['order_delivered_carrier_date'].dt.date).dt.days

# Now calssify the duration into 'OnTime/Early' & 'Late'
df['seller_to_carrier_status'] = df['seller_to_carrier_status'].apply(lambda x : 'OnTime/Early' if x >=0 else 'Late')

#Shipping status from Carrier to Customer

# First get difference between estimated delivery date and actual delivery date in days
df['arrival_status'] = (df['order_estimated_delivery_date'].dt.date - df['order_delivered_customer_date'].dt.date).dt.days

# Now Classify the duration in 'OnTime/Early' & 'Late'
df['arrival_status'] = df['arrival_status'].apply(lambda x : 'OnTime/Early' if x >=0 else 'Late')

#Show statistics of new Features

df[['estimated_days', 'arrival_days', 'shipping_days']].describe()

#Remove Outliers in both features ( More than 60 days )
outlier_indices = df[(df.estimated_days > 60) | (df.arrival_days > 60) | (df.shipping_days > 60)].index

df.drop(outlier_indices, inplace= True)
df.reset_index(inplace= True, drop= True)

#Rating estimated delivery time

def rates(x):

    if x in range(0, 8):
        return 'Very Fast'
    
    elif x in range(8, 16):
        return 'Fast'
    
    elif x in range(16, 25):
        return 'Neutral'
    
    elif x in range(25, 40):
        return 'Slow'
    
    else:
        return 'Very Slow'

df['estimated_delivery_rate'] = df.estimated_days.apply(rates)

df['arrival_delivery_rate'] = df.arrival_days.apply(rates)

df['shipping_delivery_rate'] = df.shipping_days.apply(rates)

"""Data preprocessing

Drop unneccessary features
"""

# Drop all ids, zip codes, datetimes, review comment and title, product length

df.drop(['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_id', 'order_purchase_timestamp',
        'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date',
        'review_id', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp', 'payment_sequential',
        'order_item_id', 'product_id', 'seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state', 'shipping_limit_date', 'product_category_name',
        'product_category_name_english', 'product_category', 'product_weight_g', 'product_name_lenght',
        'product_vol_cm3'], axis= 1, inplace= True)

# Show Correlation between Features
plt.figure(figsize= [10, 6])
sns.heatmap(df.corr(), annot= True)

# Remove features with high correlations
df.drop(['shipping_days', 'price'], axis= 1, inplace= True)

df.head()

#Convert Review Score from Multiclass to Binary
encoded_class = { 1 : 'Not Satisfied',
                  2 : 'Not Satisfied', 
                  3 : 'Not Satisfied', 
                  4 : 'Satisfied', 
                  5 : 'Satisfied'}

df['review_score'] = df['review_score'].map(encoded_class)

#Split Data into Input Features & Target Variable

X = df.drop('review_score', axis=1)
y = df['review_score']

#Handling Categorical Features
# Handling Ordinal Features ( Label Encoding)

labels = {'Very Slow' : 1, 
          'Slow' : 2, 
          'Neutral' : 3, 
          'Fast' : 4, 
          'Very Fast' : 5}

X.estimated_delivery_rate = X.estimated_delivery_rate.map(labels)
X.shipping_delivery_rate = X.shipping_delivery_rate.map(labels)
X.arrival_delivery_rate = X.arrival_delivery_rate.map(labels)

# Handling Nominal Features ( One Hot Encoding )

X = pd.get_dummies(X, drop_first=True)

# Split Data into Train & Test

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, stratify= y)

# Feature Selection

from sklearn.feature_selection import mutual_info_classif, SelectKBest
fs = SelectKBest(mutual_info_classif, k= 'all')
fs.fit(x_train, y_train)
x_train_fs = fs.transform(x_train)
x_test_fs = fs.transform(x_test)

# Plotting Featres as per importance

# Get the indices sorted by most important to least important
plt.figure(figsize=[15, 8])
indices = np.argsort(fs.scores_)[::-1]

# To get your top 10 feature names
features = []
for i in range(15):
    features.append(fs.feature_names_in_[indices[i]])

# Now plot
sns.barplot(x = fs.scores_[indices[range(15)]], y = features)

# Select best 9 Features

from sklearn.feature_selection import mutual_info_classif, SelectKBest
fs = SelectKBest(mutual_info_classif, k= 9)
fs.fit(x_train, y_train)
x_train_fs = fs.transform(x_train)
x_test_fs = fs.transform(x_test)

x_train_fs = pd.DataFrame(x_train_fs, columns= fs.get_feature_names_out())
x_test_fs = pd.DataFrame(x_test_fs, columns= fs.get_feature_names_out())

# Feature Scaling

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean= False)
x_train_scaled = sc.fit_transform(x_train_fs)
x_test_scaled = sc.transform(x_test_fs)

# Convert Array to Dataframe

x_train_scaled = pd.DataFrame(x_train_scaled, columns= sc.get_feature_names_out())
x_test_scaled = pd.DataFrame(x_test_scaled, columns= sc.get_feature_names_out())

# Handling Imbalance

# Check imbalance percentage

round((y_train.value_counts() / y_train.shape[0]) * 100, 2)

# Use SMOTE for handling imbalance

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state= 42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train_scaled, y_train)

"""Modeling"""



# Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

lr = LogisticRegression()
lr.fit(x_train_resampled, y_train_resampled)

print('Evaluation on Training \n', classification_report(y_train_resampled, lr.predict(x_train_resampled)))
print('Evaluation on Testing \n', classification_report(y_test, lr.predict(x_test_scaled)))

# KNN Classifier

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(x_train_resampled, y_train_resampled)

print('Evaluation on Training \n', classification_report(y_train_resampled, knn.predict(x_train_resampled)))
print('Evaluation on Testing \n', classification_report(y_test, knn.predict(x_test_scaled)))

# Decision Tree

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(x_train_resampled, y_train_resampled)

print('Evaluation on Training \n', classification_report(y_train_resampled, dt.predict(x_train_resampled)))
print('Evaluation on Testing \n', classification_report(y_test, dt.predict(x_test_scaled)))

# Random Forest

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(x_train_resampled, y_train_resampled)

print('Evaluation on Training \n', classification_report(y_train_resampled, rf.predict(x_train_resampled)))
print('Evaluation on Testing \n', classification_report(y_test, rf.predict(x_test_scaled)))

# Naive Bayes

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(x_train_resampled, y_train_resampled)
y_pred = nb.predict(x_test_scaled)

print('Evaluation on Training \n', classification_report(y_train_resampled, nb.predict(x_train_resampled)))
print('Evaluation on Testing \n', classification_report(y_test, nb.predict(x_test_scaled)))

"""Pipeline"""

df_pipeline = df.copy()
df_pipeline.head()

# Encoding Review score to 0 and 1

encoded_class = { 'Not Satisfied' : 0,
                  'Satisfied' : 1,
                }

df_pipeline['review_score'] = df_pipeline['review_score'].map(encoded_class)

# Split Input Features and Targe Variable

X = df_pipeline.drop('review_score', axis=1)
y = df_pipeline['review_score']

# Split into Train & Test
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42, stratify= y)

# Prepare Numerical Features

numeric_columns = x_train.select_dtypes(exclude = 'object').columns
numeric_columns

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numerical_pipeline = Pipeline(steps=[('Handle Missing Values', SimpleImputer(strategy= 'median')), 
                                    ('Feature Scaling', StandardScaler(with_mean=False))])

# Prepare Categorical Features

cat_columns = x_train.select_dtypes(include = 'object').columns
cat_columns

cat_pipeline = Pipeline(steps=[('Handle Missing Values', SimpleImputer(strategy= 'most_frequent')),
                                ('OneHot Encoding', OneHotEncoder(drop= 'first')),
                                ('Feature Scaling', StandardScaler(with_mean= False))])

from sklearn.compose import ColumnTransformer

preprocessing = ColumnTransformer(transformers=[('Numerical Columns', numerical_pipeline, numeric_columns),
                                                ('Cat Columns', cat_pipeline, cat_columns)], remainder= 'passthrough')
preprocessing

from xgboost import XGBClassifier
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

final_pipeline = Pipeline(steps=[('Preprocessing', preprocessing), ('Smote', SMOTE()), 
                                ('Model', XGBClassifier(learning_rate= 0.2, max_depth= 8, n_estimators= 200))])
final_pipeline

final_pipeline.fit(x_train, y_train)

import joblib
joblib.dump(final_pipeline, 'Brazilian Ecommerce Classification.bkl')

# Prepare Features

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer

numerical_pipeline_cluster = Pipeline(steps=[('Feature Scaling', StandardScaler(with_mean=False))])

# Create copy of DataFrame
df_2 = df.copy()

df_2.head()

df_cluster = df_2[['freight_value', 'payment_value', 'payment_installments']]

# Take another sample of original cluster dataframe to assign kmeans labels
original_cluster_sample = df_cluster.sample(frac= 1, random_state= 42)[:9966]

df_sample = df_cluster.sample(frac= 1, random_state= 42)[:10000]

cluster_pipeline = df_sample.copy()

for i in ['freight_value', 'payment_value', 'payment_installments']:
    df_sample[i] = np.log10(df_sample[i])

from sklearn.compose import ColumnTransformer

preprocessing_cluster = ColumnTransformer(transformers= [('Numerical Columns', numerical_pipeline_cluster, cluster_pipeline.columns)], 
                                          remainder= 'passthrough')
preprocessing_cluster

from sklearn.cluster import KMeans
final_pipeline_cluster = Pipeline(steps=[('Preprocessing', preprocessing_cluster), ('Log Transformer', FunctionTransformer(np.log10)),
                                ('Model', KMeans(n_clusters= 3))])
final_pipeline_cluster

# Save model as bkl file
import joblib
joblib.dump(final_pipeline_cluster, 'Brazilian Ecommerce Clustering.bkl')

"""Model Deployment"""

model_classification = joblib.load('Brazilian Ecommerce Classification.bkl')
model_clustering = joblib.load('Brazilian Ecommerce Clustering.bkl')

!pip install -q streamlit

# Install neccessary libraries for deployment

! pip install ydata_profiling
! pip install streamlit_pandas_profiling

pip install pyngrok

from pyngrok import ngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py 
# import streamlit as st 
# st.markdown(""" This is a Streamlit App """)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile Brazilian_Ecommerce_Project.py
# 
# import numpy as np
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# import joblib
# import streamlit as st
# from sklearn.preprocessing import  StandardScaler
# from sklearn.cluster import KMeans
# from sklearn.cluster import AgglomerativeClustering
# from sklearn.decomposition import PCA
# from ydata_profiling import ProfileReport
# from streamlit_pandas_profiling import st_profile_report
# 
# # Load Classification and Clustering Pipeline models
# model_classification = joblib.load('Brazilian Ecommerce Classification.bkl')
# model_clustering = joblib.load('Brazilian Ecommerce Clustering.bkl')
# 
# # Create Sidebar to navigate between EDA, Classification and Clustering
# sidebar = st.sidebar
# mode = sidebar.radio('Mode', ['EDA', 'Classification', 'Clustering'])
# st.markdown("<h1 style='text-align: center; color: #ff0000;'></h1>", unsafe_allow_html=True)
# 
# if mode == "EDA":
# 
#     def main():
# 
#         # Header of Customer Satisfaction Prediction
#         html_temp="""
#                     <div style="background-color:#F5F5F5">
#                     <h1 style="color:#31333F;text-align:center;"> Customer Satisfaction Prediction </h1>
#                     </div>
#                 """
#         # Create sidebar to upload CSV files
#         with st.sidebar.header('Upload your CSV data'):
#             uploaded_file = st.sidebar.file_uploader('Upload your input csv file')
# 
#          if uploaded_file is not None:
#             # Read file and Put headers
#             EDA_sample = pd.read_csv(uploaded_file, index_col= 0)
#             pr = ProfileReport(EDA_sample, explorative=True)
#             st.header('**Input DataFrame**')
#             st.write(EDA_sample)
#             st.write('---')
#             st.header('**Pandas Profiling Report**')
#             st_profile_report(pr)
#         
#         else:
#             st.info('Awaiting for CSV file to be uploaded.')
# 
#     if __name__ == '__main__':
#         main()
# 
# if mode == "Classification":
# 
#     # Define function to predict classification based on assigned features
#     def predict_satisfaction(freight_value, product_description_lenght, product_photos_qty, payment_type, payment_installments, payment_value, 
#     estimated_days, arrival_days, arrival_status, seller_to_carrier_status, estimated_delivery_rate, arrival_delivery_rate, shipping_delivery_rate):
# 
#         prediction_classification = model_classification.predict(pd.DataFrame({'freight_value' :[freight_value], 'product_description_lenght' :[product_description_lenght], 'product_photos_qty' :[product_photos_qty], 'payment_type' :[payment_type], 'payment_installments' :[payment_installments], 'payment_value' :[payment_value], 'estimated_days' :[estimated_days], 'arrival_days' :[arrival_days], 'arrival_status' :[arrival_status], 'seller_to_carrier_status' :[seller_to_carrier_status], 'estimated_delivery_rate' :[estimated_delivery_rate], 'arrival_delivery_rate' :[arrival_delivery_rate], 'shipping_delivery_rate' :[shipping_delivery_rate]}))
#         return prediction_classification
# 
#     def main():
# 
#         # Header of Customer Satisfaction Prediction
#         html_temp="""
#                     <div style="background-color:#F5F5F5">
#                     <h1 style="color:#31333F;text-align:center;"> Customer Satisfaction Prediction </h1>
#                     </div>
#                 """
#         st.markdown(html_temp,unsafe_allow_html=True)
#         
#         # Assign all features with desired data input method
#         sidebar.title('Numerical Features')
#         product_description_lenght = sidebar.slider('product_description_lenght', 4,3990,100)
#         product_photos_qty = sidebar.slider('product_photos_qty', 1,20,1)
#         payment_installments = sidebar.slider('payment_installments', 1,24,1)
#         estimated_days = sidebar.slider('estimated_days', 3,60,1)
#         arrival_days = sidebar.slider('arrival_days', 0,60,1)
#         payment_type = st.selectbox('payment_type', ['credit_card', 'boleto', 'voucher', 'debit_card'])
#         arrival_status = st.selectbox('arrival_status', ['OnTime/Early', 'Late'])
#         seller_to_carrier_status = st.selectbox('seller_to_carrier_status', ['OnTime/Early', 'Late'])
#         estimated_delivery_rate = st.selectbox('estimated_delivery_rate', ['Very Slow', 'Slow', 'Neutral', 'Fast', 'Very Fast'])
#         arrival_delivery_rate = st.selectbox('arrival_delivery_rate', ['Very Slow', 'Slow', 'Neutral', 'Fast', 'Very Fast'])
#         shipping_delivery_rate = st.selectbox('shipping_delivery_rate Date', ['Very Slow', 'Slow', 'Neutral', 'Fast', 'Very Fast'])
#         payment_value = st.text_input('payment_value', '')
#         freight_value = st.text_input('freight_value', '')
#         result = ''
# 
#         # Predict Customer Satsifaction
#         if st.button('Predict_Satisfaction'):
#             result = predict_satisfaction(freight_value, product_description_lenght, product_photos_qty, payment_type, payment_installments, payment_value, 
#                                         estimated_days, arrival_days, arrival_status, seller_to_carrier_status, estimated_delivery_rate, arrival_delivery_rate, shipping_delivery_rate)
#                                         
#         if result == 0:
#             result = 'Not Satisfied'
#             st.success(f'The Customer is {result}')
#         else:
#             result = 'Satisfied'
#             st.success(f'The Customer is {result}')
# 
#     if __name__ == '__main__':
#         main()
# 
# if mode == "Clustering":
# 
#     def predict_clustering(freight_value, price, payment_value, payment_installments, payment_sequential):
# 
#         prediction_clustering = model_clustering.predict(pd.DataFrame({'freight_value' :[freight_value], 'price' :[price], 'payment_installments' :[payment_installments], 'payment_value' :[payment_value], 'payment_sequential' :[payment_sequential]}))
#         return prediction_clustering
# 
#     def main():
# 
#         # Header of Customer Segmentation
#         html_temp="""
#                 <div style="background-color:#F5F5F5">
#                 <h1 style="color:#31333F;text-align:center;"> Customer Segmentation </h1>
#                 </div>
#             """
#         st.markdown(html_temp,unsafe_allow_html=True)
# 
#         # Assign all features with desired data input method
#         payment_installments = st.slider('payment_installments', 1,24,1)
#         payment_sequential = st.slider('payment_sequential', 1,24,1)
#         freight_value = st.text_input('freight_value', '')
#         price = st.text_input('price', '')
#         payment_value = st.text_input('payment_value', '')
#         result_cluster = ''
# 
#         # Predict Cluster of the customer
#         if st.button('Predict_Cluster'):
#             result_cluster = predict_clustering(freight_value, price, payment_value, payment_installments, payment_sequential)
#                                         
#         st.success(f'Customer Cluster is {result_cluster}')
#         
#         # Upload CSV file
#         with st.sidebar.header('Upload your CSV data'):
#             uploaded_file = st.sidebar.file_uploader('Upload your input csv file')
# 
#         if uploaded_file is not None:
# 
#             # Read dataset
#             sample = pd.read_csv(uploaded_file, index_col= 0)
#             
#             # Define sidebar for clustering algorithm
#             selected_algorithm = sidebar.selectbox('Select Clustering Algorithm', ['K-Means', 'Agglomerative'])
# 
#             # Define sidebar for number of clusters
#             selected_clusters = sidebar.slider('Select number of clusters', 2, 10, 1)
# 
#             # Define sidebar for PCA
#             use_pca = sidebar.radio('Use PCA', ['No', 'Yes'])
# 
#             # Drop freight values with zeros
#             sample.drop(sample[sample.freight_value == 0].index, inplace= True)
#             # Reset Index 
#             sample.reset_index(inplace= True, drop= True)
#             # Handle Skeweness in sample data
#             for i in ['freight_value', 'price', 'payment_value', 'payment_installments', 'payment_sequential']:
#                 sample[i] = np.log10(sample[i])
# 
#             # Apply standard scaler
#             sc = StandardScaler(with_mean= False)
#             data_scaled = sc.fit_transform(sample)
# 
#             # Select number of clusters
#             if selected_algorithm == 'Agglomerative':
#                 hc = AgglomerativeClustering(n_clusters= selected_clusters)
#                 y_pred_hc = hc.fit_predict(data_scaled)
# 
#             else:
#                 kmean = KMeans(n_clusters= selected_clusters)
#                 y_pred_kmean = kmean.fit_predict(data_scaled)
# 
#             # Apply PCA
#             pca = PCA(n_components= 2)
#             data_pca = pca.fit_transform(data_scaled)
# 
#             # Select number of clusters for PCA
#             kmean_pca = KMeans(n_clusters= selected_clusters)
#             y_pred_pca = kmean_pca.fit_predict(data_pca)
# 
#             def plot_cluster(data, y_pred, num_clusters):
# 
#                 # Plot Clusters
#                 fig, ax = plt.subplots()
#                 Colors= ['red', 'green', 'blue', 'purple', 'orange', 'royalblue', 'brown', 'grey', 'chocolate', 'fuchsia']
#                 for i in range(num_clusters):
#                     ax.scatter(data[y_pred==i,0], data[y_pred==i,1], c= Colors[i], label= 'Cluster ' + str(i+1))
# 
#                 ax.set_title('Customers Clusters')
#                 ax.legend(loc='upper left', prop={'size':5})
#                 ax.axis('off')
#                 st.pyplot(fig)
# 
#             # Option to select and plot PCA for clustering
#             if use_pca == 'No' and selected_algorithm == 'K-Means':
#                 plot_cluster(data_scaled, y_pred_kmean, selected_clusters)
# 
#             elif use_pca == 'No' and selected_algorithm == 'Agglomerative':
#                 plot_cluster(data_scaled, y_pred_hc, selected_clusters)           
# 
#             else:
#                 plot_cluster(data_pca, y_pred_pca, selected_clusters)    
#         
#         else:
#             st.info('Awaiting for CSV file to be uploaded.')
# 
#     if __name__ == '__main__':
#         main()
# 
# 
#         
#

!npm install localtunnel

#! streamlit run Brazilian_Ecommerce_Project.py

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

!./ngrok authtoken 2R2yN51Wxug8L8vgljGDQqa9SrL_6biYfXywfzEJCMuKK31TK

!./ngrok http 8501

!npx localtunnel --port 8501