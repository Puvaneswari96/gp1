# -*- coding: utf-8 -*-
"""Brazilian_Ecommerce_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hX3ROa1RlRfRcdz_LDtpSwVREyEOYdV1

# <center> EDA + Satisfaction Prediction + Reviews NLP + RFM Analysis + Deployment

## Table of Contents
1.0 Introduction

2.0 Data loading

3.0 Data Cleaning

        3.1 Merging ALL Dataframes

        3.2 Handling Missing Values

        3.3 Drop Duplicates

        3.4 Feature Engineering

4.0 Exploratory Data Analysis (EDA)

        6.1 Univariate Analysis

        6.2 Multivariate Analysis
    
5.0 - Data preprocesing

        6.1 Data encoding

        6.2 Feature scaling

        6.3 Handle imbalance
        
6.0 - Modeling

        6.1 Apply ML models

        6.2 Hyperparameter Tuning
        
7.0 - Pipeline

8.0 - NLP For Customer Satisfaction

9.0 - Customer Segmentation

        9.1 Customer Segmentation by RFM Analysis

        9.2 Customer Segmentation by K-Means

10.0 - Model Deployment (Classification & Clustering)

11.0 - Wrap up & Conclusion

# 1.0 Introduction
## This project is about "Olist", a Brazilian ecommerce store which has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil where its features allow viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers.

## Overview of Final Web App

<iframe width="1042" height="586" src="https://www.youtube.com/embed/O2nNUcj35ik" title="E-commerce Project Deloyment" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

# 2.0 Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""### Read All Files"""

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

customers_df= pd.read_csv('olist_customers_dataset.csv')
geolocation_df= pd.read_csv('olist_geolocation_dataset.csv')
items_df= pd.read_csv('olist_order_items_dataset.csv')
payments_df= pd.read_csv('olist_order_payments_dataset.csv')
reviews_df= pd.read_csv('olist_order_reviews_dataset.csv')
orders_df= pd.read_csv('olist_orders_dataset.csv')
products_df= pd.read_csv('olist_products_dataset.csv')
sellers_df= pd.read_csv('olist_sellers_dataset.csv')
category_translation_df= pd.read_csv('product_category_name_translation.csv')

"""### "Customers" Dataset"""

customers_df.head()

"""### "Geolocation" Dataset"""

geolocation_df.head()

"""### "Order items" Dataset"""

items_df.head()

"""### "Order Payments" Dataset"""

payments_df.head()

"""### "Order Reviews" Dataset"""

reviews_df.head()

"""### "Orders" Dataset"""

orders_df.head()

"""### "Products" Dataset"""

products_df.head()

"""### "Sellers" Dataset"""

sellers_df.head()

"""### "Product Category Name Translation" Dataset"""

category_translation_df.head()

"""# 3.0 Data Cleaning

### 3.1 Merging All Dataframes
"""

df= pd.merge(customers_df, orders_df, on="customer_id", how='inner')
df= df.merge(reviews_df, on="order_id", how='inner')
df= df.merge(items_df, on="order_id", how='inner')
df= df.merge(products_df, on="product_id", how='inner')
df= df.merge(payments_df, on="order_id", how='inner')
df= df.merge(sellers_df, on='seller_id', how='inner')
df= df.merge(category_translation_df, on='product_category_name', how='inner')
df.shape

"""### Show All Features"""

df.columns

"""#### Check duplicates"""

df.duplicated().sum()

df.describe()

df.info()

"""### 3.2 Handling Missing Values"""

# Number of Missing Values for the first half of features

df.isna().sum()[:20]

"""##### Drop All Missing Values in datetime columns"""

df.dropna(subset= ['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date'], inplace=True)

"""##### Keep " review_comment_message " & " review_comment_title "  Features ( Will be handled later )"""

# Number of Missing Values for the Second half of features

df.isna().sum()[20:]

"""##### Check the missing values"""

df[['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']][df.product_weight_g.isna()]

# Since all the missing values are in the same raw, we will drop this raw
df.drop(27352, inplace=True)

# Reset Index
df.reset_index(inplace= True, drop= True)

"""### 3.3 Feature Engineering

##### Classify Products Categories (71) into 9 main Categories
"""

def classify_cat(x):

    if x in ['office_furniture', 'furniture_decor', 'furniture_living_room', 'kitchen_dining_laundry_garden_furniture', 'bed_bath_table', 'home_comfort', 'home_comfort_2', 'home_construction', 'garden_tools', 'furniture_bedroom', 'furniture_mattress_and_upholstery']:
        return 'Furniture'
    
    elif x in ['auto', 'computers_accessories', 'musical_instruments', 'consoles_games', 'watches_gifts', 'air_conditioning', 'telephony', 'electronics', 'fixed_telephony', 'tablets_printing_image', 'computers', 'small_appliances_home_oven_and_coffee', 'small_appliances', 'audio', 'signaling_and_security', 'security_and_services']:
        return 'Electronics'
    
    elif x in ['fashio_female_clothing', 'fashion_male_clothing', 'fashion_bags_accessories', 'fashion_shoes', 'fashion_sport', 'fashion_underwear_beach', 'fashion_childrens_clothes', 'baby', 'cool_stuff', ]:
        return 'Fashion'
    
    elif x in ['housewares', 'home_confort', 'home_appliances', 'home_appliances_2', 'flowers', 'costruction_tools_garden', 'garden_tools', 'construction_tools_lights', 'costruction_tools_tools', 'luggage_accessories', 'la_cuisine', 'pet_shop', 'market_place']:
        return 'Home & Garden'
    
    elif x in ['sports_leisure', 'toys', 'cds_dvds_musicals', 'music', 'dvds_blu_ray', 'cine_photo', 'party_supplies', 'christmas_supplies', 'arts_and_craftmanship', 'art']:
        return 'Entertainment'
    
    elif x in ['health_beauty', 'perfumery', 'diapers_and_hygiene']:
        return 'Beauty & Health'
    
    elif x in ['food_drink', 'drinks', 'food']:
        return 'Food & Drinks'
    
    elif x in ['books_general_interest', 'books_technical', 'books_imported', 'stationery']:
        return 'Books & Stationery'
    
    elif x in ['construction_tools_construction', 'construction_tools_safety', 'industry_commerce_and_business', 'agro_industry_and_commerce']:
        return 'Industry & Construction'

df['product_category'] = df.product_category_name_english.apply(classify_cat)

df.product_category.value_counts()

"""##### Combine Width, Height and Length to get Product Volume"""

# Create Volume Column
df['product_vol_cm3'] = df.product_length_cm * df.product_width_cm * df.product_height_cm

# Drop Width, Height and Length
df.drop(['product_length_cm', 'product_width_cm', 'product_height_cm'], axis= 1, inplace= True)

"""#### Convert Datetime features from Object to Datetime"""

df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])
df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'])
df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'])
df['shipping_limit_date'] = pd.to_datetime(df['shipping_limit_date'])
df['order_delivered_carrier_date'] =pd.to_datetime(df['order_delivered_carrier_date'])

"""##### Extract duration of estimated shipping from purchasing date untill estimated delivery date"""

df['estimated_days'] = (df['order_estimated_delivery_date'].dt.date - df['order_purchase_timestamp'].dt.date).dt.days

"""##### Extract duration of shipping from purchasing date until delivered to customer date"""

df['arrival_days'] = (df['order_delivered_customer_date'].dt.date - df['order_purchase_timestamp'].dt.date).dt.days

"""##### Extract duration of shipping from purchasing carrier delivered date untill delivered to customer"""

df['shipping_days'] = (df['order_delivered_customer_date'].dt.date - df['order_delivered_carrier_date'].dt.date).dt.days

"""##### Drop inconsistent dates where "order_delivered_carrier_date" is greater than "order_delivered_customer_date"
"""

df.drop((df[['order_delivered_carrier_date', 'order_delivered_customer_date']][df.shipping_days < 0]).index, inplace= True)

"""##### Shipping status from Seller to Carrier"""

# First get seller to carrier duration in days
df['seller_to_carrier_status'] = (df['shipping_limit_date'].dt.date - df['order_delivered_carrier_date'].dt.date).dt.days

# Now calssify the duration into 'OnTime/Early' & 'Late'
df['seller_to_carrier_status'] = df['seller_to_carrier_status'].apply(lambda x : 'OnTime/Early' if x >=0 else 'Late')

"""##### Shipping status from Carrier to Customer"""

# First get difference between estimated delivery date and actual delivery date in days
df['arrival_status'] = (df['order_estimated_delivery_date'].dt.date - df['order_delivered_customer_date'].dt.date).dt.days

# Now Classify the duration in 'OnTime/Early' & 'Late'
df['arrival_status'] = df['arrival_status'].apply(lambda x : 'OnTime/Early' if x >=0 else 'Late')

"""#### Show statistics of new Features"""

df[['estimated_days', 'arrival_days', 'shipping_days']].describe()

"""#### Remove Outliers in both features ( More than 60 days )"""

outlier_indices = df[(df.estimated_days > 60) | (df.arrival_days > 60) | (df.shipping_days > 60)].index

df.drop(outlier_indices, inplace= True)
df.reset_index(inplace= True, drop= True)

"""##### Rating estimated delivery time"""

def rates(x):

    if x in range(0, 8):
        return 'Very Fast'
    
    elif x in range(8, 16):
        return 'Fast'
    
    elif x in range(16, 25):
        return 'Neutral'
    
    elif x in range(25, 40):
        return 'Slow'
    
    else:
        return 'Very Slow'

df['estimated_delivery_rate'] = df.estimated_days.apply(rates)

df['arrival_delivery_rate'] = df.arrival_days.apply(rates)

df['shipping_delivery_rate'] = df.shipping_days.apply(rates)

"""# 4.0 Exploratory Data Analysis ( EDA )

### 4.1 Univariate Analysis

##### Top 10 Customers Capacity Cities
"""

plt.figure(figsize=[10, 6])
sns.barplot(x = df.customer_city.value_counts().values[:10], y = df.customer_city.value_counts().index[:10], palette= 'crest_r')
plt.title('Top 10 Customers Capacity Cities')
sns.despine()

"""##### Top 10 Customers Capacity States"""

plt.figure(figsize=[10, 6])
sns.barplot(x = df.customer_state.value_counts().values[:10], y = df.customer_state.value_counts().index[:10], palette= 'crest_r')
plt.title('Top 10 Customers Capacity States')
sns.despine()

"""##### "Order_Status"
"""

df.order_status.value_counts()

"""#### Due to the Extreme imbalance and lack of variance in the feature, we should drop it"""

df.drop('order_status', axis=1, inplace=True)

"""##### " Review_Score "
"""

plt.figure(figsize=[15, 8])
review_score_index = [str(i) for i in df.review_score.value_counts().index]
sns.barplot(x = review_score_index, y= df.review_score.value_counts().values, palette= 'crest_r')
plt.title('Review Scores')
sns.despine()

"""##### " Price " """

plt.figure(figsize=[10, 6])
sns.set_palette('crest_r')
sns.distplot(x = df.price)
plt.title('Price Distribution')
sns.despine()

"""##### " Freight Value " """

plt.figure(figsize=[10, 6])
sns.set_palette('crest_r')
sns.distplot(x = df.freight_value)
plt.title('Freight Value Distribution')
sns.despine()

"""##### " Number of orders per each Category " """

plt.figure(figsize=[10, 6])
sns.barplot(x = df.product_category.value_counts().values, y = df.product_category.value_counts().index, palette= 'crest_r')
plt.title('Number of orders per each Category')
plt.xticks(rotation = 45)
sns.despine()

"""##### " Product Name Length " """

plt.figure(figsize=[10, 6])
sns.distplot(x = df.product_name_lenght)
plt.title('Product Name Length')
df.product_category.value_counts().values

"""##### " Product Description Length " """

plt.figure(figsize=[10, 6])
sns.distplot(x = df.product_description_lenght)
plt.title('Product Describtion Length')
sns.despine()

"""##### " Product Photos Quantity " """

plt.figure(figsize=[10, 6])
sns.countplot(x = df.product_photos_qty, palette= 'crest_r')
plt.title('Product Photos Quantity')
sns.despine()

"""##### " Product Weight "
"""

plt.figure(figsize=[10, 6])
sns.distplot(x = df.product_weight_g)
plt.title('Product Weight')
sns.despine()

"""##### " Product Volume "
"""

plt.figure(figsize=[10, 6])
sns.distplot(x = df.product_vol_cm3)
plt.title('Product Volume')
sns.despine()

"""##### " Payment Type "
"""

plt.figure(figsize=[10, 10])
plt.pie(df.payment_type.value_counts().values, explode=(0.05, 0.05, 0.05, 0.05), labels= df.payment_type.value_counts().index, autopct='%1.1f%%',shadow=True, startangle=90);

"""##### " Payment Installments "
"""

df.payment_installments.value_counts()

df[df.payment_installments == 0]

"""##### Since no of installments can't be 0, we should drop these raws"""

# Drop indices
df.drop([29113, 29114, 96733], inplace=True)

# Reset Index
df.reset_index(inplace= True, drop= True)

plt.figure(figsize=[10, 6])
sns.countplot(x = df.payment_installments, palette= 'crest_r')
plt.title('Installments Distribution')
sns.despine()

"""##### " Payment Value "
"""

plt.figure(figsize=[10, 6])
sns.distplot(x = df.payment_value)
plt.title('Payment Value')
sns.despine()

"""##### " Top 10 Cities for Sellers "
"""

plt.figure(figsize=[10, 6])
sns.barplot(x = df.seller_city.value_counts().values[:10], y= df.seller_city.value_counts().index[:10], palette= 'crest_r')
plt.title('Top 10 Sellers Cities')
sns.despine()

"""##### " Top 10 Sellers Capacity States  "
"""

plt.figure(figsize=[15, 8])
sns.barplot(x = df.seller_state.value_counts().values[:10], y= df.seller_state.value_counts().index[:10], palette= 'crest_r')
plt.title('Top 10 Sellers States')
sns.despine()

"""### 4.2 Multivariate Analysis

### Which Cities have highest Revenue ?
"""

# Group customer city by payment value
revenue_per_city = df.groupby('customer_city')[['payment_value']].sum().sort_values(by='payment_value', ascending=False)
revenue_per_city.reset_index(inplace=True)

# plot Top 10 cities with highest revenue
plt.figure(figsize=[15, 8])
sns.barplot(x = revenue_per_city.customer_city[:10], y= revenue_per_city.payment_value[:10], palette= 'crest_r')
plt.title('Top 10 Cities with highest Revenue', fontsize= 15)
plt.xlabel('Customer City', fontsize= 12)
plt.ylabel('Total Payments in Millions',fontsize= 12)
sns.despine()

"""### What is the average review score for each product category ?"""

# Filter product category with 4.5 or above
review_per_cat = df.groupby('product_category')[['review_score']].mean().sort_values(by='review_score', ascending=False)
review_per_cat.reset_index(inplace=True)

# Plot Product Category vs Review Score
plt.figure(figsize=[15, 8])
sns.barplot(x = review_per_cat.review_score, y= review_per_cat.product_category, palette= 'crest_r')
plt.title('Average Review Score per Product Category', fontsize= 15)
plt.xlabel('Review Score', fontsize=12)
plt.ylabel('Prodcut Category', fontsize= 12)
ax = plt.gca()
ax.set_frame_on(False);

"""### Are customers more likely to make larger payments using certain payment methods ?"""

# Group each payment type by average payment value
payment_methods = df.groupby('payment_type')[['payment_value']].sum().sort_values(by='payment_value', ascending=False)
payment_methods.reset_index(inplace=True)

# plot Average payments per payment method
plt.figure(figsize=[15, 8])
sns.barplot(x = payment_methods.payment_type, y= payment_methods.payment_value, palette= 'crest_r')
plt.title('Total Revenue per payment method', fontsize= 15)
plt.xlabel('Payment Type', fontsize= 12)
plt.ylabel('Revenue per Payment type (Millions)', fontsize= 12)
sns.despine()

# Group each payment type by average payment value
payment_methods = df.groupby('payment_type')[['payment_value']].mean().sort_values(by='payment_value', ascending=False)
payment_methods.reset_index(inplace=True)

# plot Average payments per payment method
plt.figure(figsize=[15, 8])
sns.barplot(x = payment_methods.payment_type, y= payment_methods.payment_value, palette= 'crest_r')
plt.title('Average payment value per payment method', fontsize= 15)
plt.xlabel('Payment Type', fontsize= 12)
plt.ylabel('Average Payment Value', fontsize= 12)
sns.despine()

"""### What is the average freight value for each product category ?"""

# Group product category by average freight value
freight_per_cat = df.groupby('product_category')[['freight_value']].mean().sort_values(by='freight_value', ascending=False)
freight_per_cat.reset_index(inplace=True)

# plot average freight value per product category
plt.figure(figsize=[15, 8])
sns.barplot(x = freight_per_cat.freight_value, y=  freight_per_cat.product_category, palette= 'crest_r')
plt.title('Average Freight Value per Product Category', fontsize= 15)
plt.xlabel('Average Freight Value',fontsize= 12)
plt.ylabel('Product Category', fontsize= 12)
ax = plt.gca()
ax.set_frame_on(False);

"""### What is the average shipping time for each product Category ?"""

# Group product category by average arrival time
ship_per_cat = df.groupby('product_category')[['arrival_days']].mean().sort_values(by='arrival_days', ascending=False)
ship_per_cat.reset_index(inplace=True)

# plot average freight value per product category
plt.figure(figsize=[15, 8])
sns.barplot(x = ship_per_cat.arrival_days, y=  ship_per_cat.product_category, palette= 'crest_r')
plt.title('Average arrival Time per Product Category', fontsize= 15)
plt.xlabel('Average arrival time (days)',fontsize= 12)
plt.ylabel('Product Category', fontsize= 12)
ax = plt.gca()
ax.set_frame_on(False);

"""### Are Transactions done with Vouchers lead to high review score ?"""

plt.figure(figsize=[15, 8])
voucher_trans = df[df.payment_type == 'voucher']
sns.countplot(x= voucher_trans.review_score)

"""### How accurate are the estimated delivery dates provided to customers ?"""

plt.figure(figsize=[30,8])
Values = df.arrival_status.value_counts().values
Labels = df.arrival_status.value_counts().index
plt.pie(Values, explode=(0.05, 0.05), labels= ['OnTime/Early', 'Late'], autopct='%1.1f%%', shadow=True, colors= ('#0000FF', '#C0C0C0'));

"""### Distribution of products categories by location ?"""

plt.figure(figsize=[15, 8])
sns.scatterplot(x = geolocation_df.geolocation_lng, y = geolocation_df.geolocation_lat, hue= df.product_category)
plt.title('Distribution Of Categories by location', fontsize= 15)
plt.xlabel('Longitude',fontsize= 12)
plt.ylabel('Latitude', fontsize= 12)
ax = plt.gca()
ax.set_frame_on(False);sns.despine()

# Create copy of DataFrame
df_2 = df.copy()

# Save sample for EDA Deployment
EDA_df = df_2.drop(['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'order_id', 'review_id', 'order_item_id', 'product_id', 'seller_id', 'seller_zip_code_prefix', 'product_category_name'], axis= 1)
EDA_sample = EDA_df.sample(frac= 1)[:10000]
EDA_sample.to_csv('EDA.csv')

"""# 5.0 Data Preprocessing

### 5.1 Drop Unneccessary Features
"""

# Drop all ids, zip codes, datetimes, review comment and title, product length

df.drop(['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_id', 'order_purchase_timestamp',
        'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date',
        'review_id', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp', 'payment_sequential',
        'order_item_id', 'product_id', 'seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state', 'shipping_limit_date', 'product_category_name',
        'product_category_name_english', 'product_category', 'product_weight_g', 'product_name_lenght',
        'product_vol_cm3'], axis= 1, inplace= True)

# Show Correlation between Features
plt.figure(figsize= [10, 6])
sns.heatmap(df.corr(), annot= True)

# Remove features with high correlations
df.drop(['shipping_days', 'price'], axis= 1, inplace= True)

df.head()

"""#### Convert Review Score from Multiclass to Binary"""

encoded_class = { 1 : 'Not Satisfied',
                  2 : 'Not Satisfied', 
                  3 : 'Not Satisfied', 
                  4 : 'Satisfied', 
                  5 : 'Satisfied'}

df['review_score'] = df['review_score'].map(encoded_class)

"""#### Split Data into Input Features & Target Variable"""

X = df.drop('review_score', axis=1)
y = df['review_score']

"""### 5.2 Handling Categorical Features

#### Handling Ordinal Features ( Label Encoding)
"""

labels = {'Very Slow' : 1, 
          'Slow' : 2, 
          'Neutral' : 3, 
          'Fast' : 4, 
          'Very Fast' : 5}

X.estimated_delivery_rate = X.estimated_delivery_rate.map(labels)
X.shipping_delivery_rate = X.shipping_delivery_rate.map(labels)
X.arrival_delivery_rate = X.arrival_delivery_rate.map(labels)

"""#### Handling Nominal Features ( One Hot Encoding )"""

X = pd.get_dummies(X, drop_first=True)

"""#### Split Data into Train & Test"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, stratify= y)

"""### 5.3 Feature Selection"""

from sklearn.feature_selection import mutual_info_classif, SelectKBest
fs = SelectKBest(mutual_info_classif, k= 'all')
fs.fit(x_train, y_train)
x_train_fs = fs.transform(x_train)
x_test_fs = fs.transform(x_test)

"""##### Plotting Featres as per importance"""

# Get the indices sorted by most important to least important
plt.figure(figsize=[15, 8])
indices = np.argsort(fs.scores_)[::-1]

# To get your top 10 feature names
features = []
for i in range(15):
    features.append(fs.feature_names_in_[indices[i]])

# Now plot
sns.barplot(x = fs.scores_[indices[range(15)]], y = features)

"""#### Select best 9 Features"""

from sklearn.feature_selection import mutual_info_classif, SelectKBest
fs = SelectKBest(mutual_info_classif, k= 9)
fs.fit(x_train, y_train)
x_train_fs = fs.transform(x_train)
x_test_fs = fs.transform(x_test)

x_train_fs = pd.DataFrame(x_train_fs, columns= fs.get_feature_names_out())
x_test_fs = pd.DataFrame(x_test_fs, columns= fs.get_feature_names_out())

"""### 5.4 Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean= False)
x_train_scaled = sc.fit_transform(x_train_fs)
x_test_scaled = sc.transform(x_test_fs)

"""##### Convert Array to Dataframe"""

x_train_scaled = pd.DataFrame(x_train_scaled, columns= sc.get_feature_names_out())
x_test_scaled = pd.DataFrame(x_test_scaled, columns= sc.get_feature_names_out())

"""### 5.5 Handling Imbalance

##### Check imbalance percentage
"""

round((y_train.value_counts() / y_train.shape[0]) * 100, 2)

"""##### Use SMOTE for handling imbalance"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state= 42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train_scaled, y_train)

"""# 6.0 Modeling

### 6.1 Apply ML Models

### Logistic Regression
"""

import sklearn
print(sklearn.__version__)

!pip install -U scikit-learn

import subprocess

# Install or upgrade scikit-learn
subprocess.check_call(['pip', 'install', '-U', 'scikit-learn'])

!pip install matplotlib

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Instantiate and train the logistic regression model
lr = LogisticRegression()
lr.fit(x_train_resampled, y_train_resampled)

# Evaluate on the training set
train_predictions = lr.predict(x_train_resampled)
print('Evaluation on Training:\n', classification_report(y_train_resampled, train_predictions))

# Evaluate on the testing set
test_predictions = lr.predict(x_test_scaled)
print('Evaluation on Testing:\n', classification_report(y_test, test_predictions))

# Compute and plot the confusion matrix for the training set
train_cm = confusion_matrix(y_train_resampled, train_predictions)
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

# Compute and plot the confusion matrix for the testing set
test_cm = confusion_matrix(y_test, test_predictions)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

"""### KNN Classifier"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

knn = KNeighborsClassifier()
knn.fit(x_train_resampled, y_train_resampled)

train_predictions = knn.predict(x_train_resampled)
print('Evaluation on Training:\n', classification_report(y_train_resampled, train_predictions))

test_predictions = knn.predict(x_test_scaled)
print('Evaluation on Testing:\n', classification_report(y_test, test_predictions))

train_cm = confusion_matrix(y_train_resampled, train_predictions)
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

test_cm = confusion_matrix(y_test, test_predictions)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

"""### Decision Tree"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

dt = DecisionTreeClassifier()
dt.fit(x_train_resampled, y_train_resampled)

train_predictions = dt.predict(x_train_resampled)
print('Evaluation on Training:\n', classification_report(y_train_resampled, train_predictions))

test_predictions = dt.predict(x_test_scaled)
print('Evaluation on Testing:\n', classification_report(y_test, test_predictions))

train_cm = confusion_matrix(y_train_resampled, train_predictions)
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

test_cm = confusion_matrix(y_test, test_predictions)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

"""### Random Forest"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

rf = RandomForestClassifier()
rf.fit(x_train_resampled, y_train_resampled)

train_predictions = rf.predict(x_train_resampled)
print('Evaluation on Training:\n', classification_report(y_train_resampled, train_predictions))

test_predictions = rf.predict(x_test_scaled)
print('Evaluation on Testing:\n', classification_report(y_test, test_predictions))

train_cm = confusion_matrix(y_train_resampled, train_predictions)
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

test_cm = confusion_matrix(y_test, test_predictions)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

"""### Ada Boost"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix

ad = AdaBoostClassifier()
ad.fit(x_train_resampled, y_train_resampled)

train_predictions = ad.predict(x_train_resampled)
print('Evaluation on Training:\n', classification_report(y_train_resampled, train_predictions))

test_predictions = ad.predict(x_test_scaled)
print('Evaluation on Testing:\n', classification_report(y_test, test_predictions))

train_cm = confusion_matrix(y_train_resampled, train_predictions)
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

test_cm = confusion_matrix(y_test, test_predictions)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.show()

"""### XGboost"""

import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

le = LabelEncoder()
y_train_xg = le.fit_transform(y_train_resampled)
y_test_xg = le.fit_transform(y_test)
xg = XGBClassifier()
xg.fit(x_train_resampled, y_train_xg)

train_predictions = xg.predict(x_train_resampled)
print('Evaluation on Training:\n', classification_report(y_train_xg, train_predictions))

test_predictions = xg.predict(x_test_scaled)
print('Evaluation on Testing:\n', classification_report(y_test_xg, test_predictions))

train_cm = confusion_matrix(y_train_xg, train_predictions)
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks(np.arange(len(le.classes_)), le.classes_)
plt.yticks(np.arange(len(le.classes_)), le.classes_)
plt.show()

test_cm = confusion_matrix(y_test_xg, test_predictions)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks(np.arange(len(le.classes_)), le.classes_)
plt.yticks(np.arange(len(le.classes_)), le.classes_)
plt.show()

"""### Naive Bayes"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix

nb = GaussianNB()
nb.fit(x_train_resampled, y_train_resampled)
y_pred = nb.predict(x_test_scaled)

print('Evaluation on Training:\n', classification_report(y_train_resampled, nb.predict(x_train_resampled)))
print('Evaluation on Testing:\n', classification_report(y_test, y_pred))

train_cm = confusion_matrix(y_train_resampled, nb.predict(x_train_resampled))
plt.imshow(train_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Training Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks(np.arange(len(np.unique(y_train_resampled))), np.unique(y_train_resampled))
plt.yticks(np.arange(len(np.unique(y_train_resampled))), np.unique(y_train_resampled))
plt.show()

test_cm = confusion_matrix(y_test, y_pred)
plt.imshow(test_cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Testing Set')
plt.colorbar()
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.xticks(np.arange(len(np.unique(y_test))), np.unique(y_test))
plt.yticks(np.arange(len(np.unique(y_test))), np.unique(y_test))
plt.show()

! pip install catboost

"""# 7.0 Pipeline"""

df_pipeline = df.copy()
df_pipeline.head()

"""#### Encoding Review score to 0 and 1"""

encoded_class = { 'Not Satisfied' : 0,
                  'Satisfied' : 1,
                }

df_pipeline['review_score'] = df_pipeline['review_score'].map(encoded_class)

"""### Split Input Features and Targe Variable"""

X = df_pipeline.drop('review_score', axis=1)
y = df_pipeline['review_score']

"""### Split into Train & Test"""

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42, stratify= y)

"""### Prepare Numerical Features"""

numeric_columns = x_train.select_dtypes(exclude = 'object').columns
numeric_columns

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numerical_pipeline = Pipeline(steps=[('Handle Missing Values', SimpleImputer(strategy= 'median')), 
                                    ('Feature Scaling', StandardScaler(with_mean=False))])

"""### Prepare Categorical Features"""

cat_columns = x_train.select_dtypes(include = 'object').columns
cat_columns

cat_pipeline = Pipeline(steps=[('Handle Missing Values', SimpleImputer(strategy= 'most_frequent')),
                                ('OneHot Encoding', OneHotEncoder(drop= 'first')),
                                ('Feature Scaling', StandardScaler(with_mean= False))])

from sklearn.compose import ColumnTransformer

preprocessing = ColumnTransformer(transformers=[('Numerical Columns', numerical_pipeline, numeric_columns),
                                                ('Cat Columns', cat_pipeline, cat_columns)], remainder= 'passthrough')
preprocessing

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

final_pipeline = Pipeline(steps=[('Preprocessing', preprocessing), ('Smote', SMOTE()), 
                                ('Model', XGBClassifier(learning_rate= 0.2, max_depth= 8, n_estimators= 200))])
final_pipeline

final_pipeline.fit(x_train, y_train)

import joblib
joblib.dump(final_pipeline, 'Brazilian Ecommerce Classification.bkl')

"""# 8.0 NLP For Customer Satisfaction"""

reviews_df.head()

# Remove 'review_comment_title' because of high missing values perentage and remove other features for unneccessity
reviews_df = reviews_df[['review_comment_message', 'review_score']]

reviews_df.info()

# Drop missing values
reviews_df.dropna(inplace= True)

# Rename columns for ease
reviews_df.rename(columns = {'review_comment_message' : 'comment', 'review_score' : 'score'}, inplace= True)

# Reset index
reviews_df.reset_index(inplace= True, drop= True)

# Encode scores to be Satisfied or Not Satisfied
encoded_class = { 1 : 'Not Satisfied',
                  2 : 'Not Satisfied', 
                  3 : 'Not Satisfied', 
                  4 : 'Satisfied', 
                  5 : 'Satisfied'}

reviews_df['score'] = reviews_df['score'].map(encoded_class)

"""#### Text Cleaning & Processing"""

!pip install nltk

import nltk

# Set the download directory path
nltk.data.path.append("/path/to/nltk_data")

# Download the stopwords corpus
nltk.download('stopwords', download_dir="/path/to/nltk_data")

from sklearn.feature_extraction.text import TfidfVectorizer

# Use TFIDF Vectorizer to convert text into numbers
tf = TfidfVectorizer()
df_new = tf.fit_transform(corpus).toarray()

df_new = pd.DataFrame(df_new, columns= tf.get_feature_names_out())
df_new

"""# 9.0 Customer Segmentation

### 9.1 Customer Segmentation by RFM Analysis
"""

df_2.head()

# Get last transaction date to help calculate Recency
max_trans_date = max(df_2.order_purchase_timestamp).date()
max_trans_date

"""#### Create Recency, Frequancy and Monetary Features"""

from datetime import datetime

rfm_table = df_2.groupby('customer_unique_id').agg({'order_purchase_timestamp': lambda x:(datetime.strptime(str(max_trans_date),'%Y-%m-%d') - x.max()).days,
                                                                'product_id': lambda x:len(x),
                                                             'payment_value': lambda x:sum(x)})
rfm_table

# Rename columns
rfm_table.rename(columns={'order_purchase_timestamp':'Recency','product_id':'Frequancy','payment_value':'Monetary'}, inplace=True)
rfm_table

"""#### Create Recency, Frequancy and Monetary scores"""

rfm_table['r_score'] = pd.qcut(rfm_table['Recency'], 4, ['4','3','2','1'])
rfm_table['f_score'] = pd.qcut(rfm_table['Frequancy'].rank(method= 'first'), 4, ['1','2','3','4'])
rfm_table['m_score'] = pd.qcut(rfm_table['Monetary'], 4, ['1','2','3','4'])
rfm_table

plt.figure(figsize= [10, 6])
sns.scatterplot(x= 'Recency',y= 'Monetary', data=rfm_table)
plt.title('Recency vs Monetary', fontsize= 15)
sns.despine()

plt.figure(figsize= [10, 6])
sns.scatterplot(x='Frequancy', y='Monetary', data=rfm_table)
plt.title('Frequancy vs Monetary', fontsize= 15)
sns.despine()

"""#### Calculate RFM Score"""

rfm_table['rfm_score'] = 100 * rfm_table['r_score'].astype(int) + 10 * rfm_table['f_score'].astype(int)+ rfm_table['m_score'].astype(int)
rfm_table

"""#### Cluster customers based on RFM Score"""

def customer_segmenation(rfm_score):
  
  if rfm_score == 444:
    return 'VIP'
  
  elif  rfm_score >= 433 and rfm_score < 444:
    return 'Very Loyal'
  
  elif   rfm_score >=421 and rfm_score< 433:
    return 'Potential Loyalist'
  
  elif rfm_score>=344 and rfm_score < 421:
    return 'New customers'
  
  elif rfm_score>=323 and rfm_score<344:
    return 'Potential customer'
  
  elif rfm_score>=224 and rfm_score<311:
    return 'High risk to churn' 
  
  else:
    return 'Lost customers'       
  
rfm_table['customer_segmentation'] = rfm_table['rfm_score'].apply(customer_segmenation)

rfm_table

# Plot frquency of each segment
plt.figure(figsize=[10,6])
sns.barplot(x = rfm_table.customer_segmentation.value_counts().values, y= rfm_table.customer_segmentation.value_counts().index, palette= 'crest_r')
sns.despine()

"""### Check Outliers"""

rfm_table.describe()

"""#### Recency"""

sns.boxplot(x= rfm_table.Recency)
sns.stripplot(x = rfm_table.Recency, color= 'black')

"""#### Frequancy"""

sns.boxplot(x= rfm_table.Frequancy)
sns.stripplot(x = rfm_table.Frequancy, color= 'black')

"""#### Monetary"""

sns.boxplot(x= rfm_table.Monetary)
sns.stripplot(x = rfm_table.Monetary, color= 'black')

"""#### Remove Extreme 5% of Outliers"""

print('Recency 5% Outliers Limits:', np.percentile(rfm_table.Recency, 5), np.percentile(rfm_table.Recency, 95))
print('Frequancy 5% Outliers Limits:', np.percentile(rfm_table.Frequancy, 5), np.percentile(rfm_table.Frequancy, 95))
print('Monetary 5% Outliers Limits:', np.percentile(rfm_table.Monetary, 5), np.percentile(rfm_table.Monetary, 95))

"""#### Remove Outliers for Recency & Monetary (Extreme 5%)"""

for i in [0, 2]:

    outlier_indices = []
    col = rfm_table.columns[i]
    percentile_5 = np.percentile(rfm_table[col], 5)
    percentile_95 = np.percentile(rfm_table[col], 95)
    outlier_indices.append(rfm_table[(rfm_table[col] < percentile_5) | (rfm_table[col] > percentile_95)].index)

rfm_table.drop(outlier_indices[0][:], inplace= True)
rfm_table.reset_index(inplace= True, drop= True)

! pip install squarify

"""#### Customer Segmentation Grid"""

import squarify

plt.figure(figsize=[15,8])
plt.rc('font', size=15)

Sizes = rfm_table.groupby('customer_segmentation')[['Monetary']].count()
squarify.plot(sizes= Sizes.values, label= Sizes.index, color=["red", "orange", "blue", "yellow", "fuchsia", "green", "royalblue"], alpha=.55)
plt.suptitle("Customer Segmentation Grid", fontsize=25);

"""#### Recency & Monetary Plot"""

plt.figure(figsize= [15, 8])
colors = ['purple', 'green', 'red', 'blue', 'orange', 'royalblue', 'yellow']
sns.scatterplot(x= rfm_table.Recency, y= rfm_table.Monetary, hue= rfm_table.customer_segmentation, palette= colors)
plt.legend(prop={'size':10})
sns.despine()

"""### Check Skeweness"""

# Recency
sns.distplot(x= rfm_table.Recency)

# Frequancy
sns.distplot(x= rfm_table.Frequancy)

# Monetary
sns.distplot(x= rfm_table.Monetary)

"""#### Apply Log function to handle skeweness for Frequancy & Monetary"""

for i in ['Frequancy', 'Monetary']:
    rfm_table[i] = np.log10(rfm_table[i])

# Frequancy
sns.distplot(x= rfm_table.Frequancy)

# Monetary
sns.distplot(x= rfm_table.Monetary)

"""### 9.2 Clustering with K-means"""

df_cluster = df_2[['freight_value', 'price', 'payment_value', 'payment_installments', 'payment_sequential']]
df_cluster

"""### Take sample from data (10k)"""

df_sample = df_cluster.sample(frac= 1, random_state= 42)[:10000]

"""#### Save sample as CSV for deployment"""

df_sample.to_csv('Clustering Sample.csv')

df_sample.describe()

"""#### Drop freight values with zeros"""

df_sample.drop(df_sample[df_sample.freight_value == 0].index, inplace= True)
df_sample.reset_index(inplace= True, drop= True)

"""#### Take copy for Pipeline"""

cluster_pipeline = df_sample.copy()

for i in ['freight_value', 'price', 'payment_value', 'payment_installments', 'payment_sequential']:
    df_sample[i] = np.log10(df_sample[i])

"""### Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean= False)
data_scaled = sc.fit_transform(df_sample)

"""#### Detecting number of clusters uning Elbow Method"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
wcss = []
scores = []

for i in range(2,10):
  kmean = KMeans(n_clusters=i)
  y_pred_kmean = kmean.fit_predict(data_scaled)
  wcss.append(kmean.inertia_)
  scores.append(silhouette_score(data_scaled,y_pred_kmean))

plt.plot(range(2,10),wcss)
plt.title('number of cluster vs WCSS')
plt.xlabel('number of cluster')
plt.ylabel('WCSS')

"""#### Detecting number of clusters using Silhouete Score"""

plt.plot(range(2,10),scores)
plt.title('number of cluster vs silhouette_score')
plt.xlabel('number of cluster')
plt.ylabel('silhouette_score')

"""#### Select number of clusters k= 3"""

from sklearn.cluster import KMeans

kmean = KMeans(n_clusters= 3)
y_pred_kmean = kmean.fit_predict(data_scaled)

# Count of each cluster
len(kmean.labels_[kmean.labels_ == 0]), len(kmean.labels_[kmean.labels_ == 1]), len(kmean.labels_[kmean.labels_ == 2])

# Take another sample of original cluster dataframe to assign kmeans labels
original_cluster_sample = df_cluster.sample(frac= 1, random_state= 42)[:9966]

# Assign cluster label to original cluster sample
original_cluster_sample['cluster_label'] = y_pred_kmean
original_cluster_sample.head()

original_cluster_sample.groupby('cluster_label').describe().T

sns.pairplot(data= original_cluster_sample, hue= 'cluster_label', palette= ['blue', 'orange', 'red'])

"""#### As we can see from statistics table and pairplot that clusters have high percentage of overlaping, sow RFM would be better in this case to cluster customers.

#### Show Kmeans Clusters
"""

plt.figure(figsize=[10, 6])
plt.scatter(data_scaled[y_pred_kmean==0,0], data_scaled[y_pred_kmean==0,1], c = 'red',label = 'cluster1')
plt.scatter(data_scaled[y_pred_kmean==1,0], data_scaled[y_pred_kmean==1,1], c = 'green',label = 'cluster2')
plt.scatter(data_scaled[y_pred_kmean==2,0], data_scaled[y_pred_kmean==2,1], c = 'blue',label = 'cluster3')
plt.scatter(kmean.cluster_centers_[:,0], kmean.cluster_centers_[:,1], c='yellow', s= 100,label= 'Centroids')
plt.title('Customers Kmeans Clusters')
plt.legend()
sns.despine()

"""#### Show Clusters using PCA"""

from sklearn.decomposition import PCA
pca = PCA(n_components= 2)
x_pca = pca.fit_transform(data_scaled)
pca.explained_variance_ratio_

wcss = []
scores = []
for i in range(2,10):
  kmean = KMeans(n_clusters=i)
  y_pred = kmean.fit_predict(x_pca)
  wcss.append(kmean.inertia_)
  scores.append(silhouette_score(x_pca,y_pred))
plt.plot(range(2,10),wcss)
plt.title('Elbow method')
plt.xlabel('number of clusters')
plt.ylabel('WCSS')

plt.plot(range(2,10),scores)
plt.title('silhouette_score')
plt.xlabel('number of clusters')
plt.ylabel('silhouette_score')

kmean = KMeans(n_clusters=3)
y_pred_pca = kmean.fit_predict(x_pca)

plt.figure(figsize=[10, 6])
plt.scatter(x_pca[y_pred_pca==0,0],x_pca[y_pred_pca==0,1],c = 'red',label = 'cluster1')
plt.scatter(x_pca[y_pred_pca==1,0],x_pca[y_pred_pca==1,1],c = 'green',label = 'cluster2')
plt.scatter(x_pca[y_pred_pca==2,0],x_pca[y_pred_pca==2,1],c = 'blue',label = 'cluster3')
plt.scatter(kmean.cluster_centers_[:,0],kmean.cluster_centers_[:,1],c='yellow',s=100,label='Centroids')
plt.title('Customers Clusters with PCA')
plt.legend()
sns.despine()

"""## 9.3 Pipeline

#### Prepare Features
"""

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer

numerical_pipeline_cluster = Pipeline(steps=[('Feature Scaling', StandardScaler(with_mean=False))])

from sklearn.compose import ColumnTransformer

preprocessing_cluster = ColumnTransformer(transformers= [('Numerical Columns', numerical_pipeline_cluster, cluster_pipeline.columns)], 
                                          remainder= 'passthrough')
preprocessing_cluster

final_pipeline_cluster = Pipeline(steps=[('Preprocessing', preprocessing_cluster), ('Log Transformer', FunctionTransformer(np.log10)),
                                ('Model', KMeans(n_clusters= 3))])
final_pipeline_cluster

# Fit pipeline to Dataframe
final_pipeline_cluster.fit(cluster_pipeline)

# Save model as bkl file
import joblib
joblib.dump(final_pipeline_cluster, 'Brazilian Ecommerce Clustering.bkl')

final_pipeline_cluster

"""# 10.0 Model Deployment"""

model_classification = joblib.load('Brazilian Ecommerce Classification.bkl')
model_clustering = joblib.load('Brazilian Ecommerce Clustering.bkl')

"""#### Test Classification Model"""

model_classification.predict(pd.DataFrame({'freight_value' :[30], 'product_description_lenght' :[1000], 'product_photos_qty' :[4], 'payment_type' :['credit_card'], 'payment_installments' :[6], 'payment_value' :[1000], 'estimated_days' :[7], 'arrival_days' :[6], 'arrival_status' :['OnTime/Early'], 'seller_to_carrier_status' :['OnTime/Early'], 'estimated_delivery_rate' :['Very Fast'], 'arrival_delivery_rate' :['Very Fast'], 'shipping_delivery_rate' :['Very Fast']}))

"""#### Test Clustering Model"""

model_clustering.predict(pd.DataFrame({'freight_value' :[10], 'price' :[90], 'payment_value' :[100], 'payment_installments' :[10], 'payment_sequential' :[3]}))

# Install neccessary libraries for deployment

! pip install ydata_profiling
! pip install streamlit_pandas_profiling

"""### Deployment with Streamlit"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile Brazilian_Ecommerce_Project.py
# 
# import numpy as np
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# import joblib
# import streamlit as st
# from sklearn.preprocessing import  StandardScaler
# from sklearn.cluster import KMeans
# from sklearn.cluster import AgglomerativeClustering
# from sklearn.decomposition import PCA
# from ydata_profiling import ProfileReport
# from streamlit_pandas_profiling import st_profile_report
# 
# # Load Classification and Clustering Pipeline models
# model_classification = joblib.load('Brazilian Ecommerce Classification.bkl')
# model_clustering = joblib.load('Brazilian Ecommerce Clustering.bkl')
# 
# # Create Sidebar to navigate between EDA, Classification and Clustering
# sidebar = st.sidebar
# mode = sidebar.radio('Mode', ['EDA', 'Classification', 'Clustering'])
# st.markdown("<h1 style='text-align: center; color: #ff0000;'></h1>", unsafe_allow_html=True)
# 
# if mode == "EDA":
# 
#     def main():
# 
#         # Header of Customer Satisfaction Prediction
#         html_temp="""
#                     <div style="background-color:#F5F5F5">
#                     <h1 style="color:#31333F;text-align:center;"> Customer Satisfaction Prediction </h1>
#                     </div>
#                 """
#         # Create sidebar to upload CSV files
#         with st.sidebar.header('Upload your CSV data'):
#             uploaded_file = st.sidebar.file_uploader('Upload your input csv file')
# 
#         if uploaded_file is not None:
#             # Read file and Put headers
#             EDA_sample = pd.read_csv(uploaded_file, index_col= 0)
#             pr = ProfileReport(EDA_sample, explorative=True)
#             st.header('**Input DataFrame**')
#             st.write(EDA_sample)
#             st.write('---')
#             st.header('**Pandas Profiling Report**')
#             st_profile_report(pr)
#         
#         else:
#             st.info('Awaiting for CSV file to be uploaded.')
# 
#     if __name__ == '__main__':
#         main()
# 
# if mode == "Classification":
# 
#     # Define function to predict classification based on assigned features
#     def predict_satisfaction(freight_value, product_description_lenght, product_photos_qty, payment_type, payment_installments, payment_value, 
#     estimated_days, arrival_days, arrival_status, seller_to_carrier_status, estimated_delivery_rate, arrival_delivery_rate, shipping_delivery_rate):
# 
#         prediction_classification = model_classification.predict(pd.DataFrame({'freight_value' :[freight_value], 'product_description_lenght' :[product_description_lenght], 'product_photos_qty' :[product_photos_qty], 'payment_type' :[payment_type], 'payment_installments' :[payment_installments], 'payment_value' :[payment_value], 'estimated_days' :[estimated_days], 'arrival_days' :[arrival_days], 'arrival_status' :[arrival_status], 'seller_to_carrier_status' :[seller_to_carrier_status], 'estimated_delivery_rate' :[estimated_delivery_rate], 'arrival_delivery_rate' :[arrival_delivery_rate], 'shipping_delivery_rate' :[shipping_delivery_rate]}))
#         return prediction_classification
# 
#     def main():
# 
#         # Header of Customer Satisfaction Prediction
#         html_temp="""
#                     <div style="background-color:#F5F5F5">
#                     <h1 style="color:#31333F;text-align:center;"> Customer Satisfaction Prediction </h1>
#                     </div>
#                 """
#         st.markdown(html_temp,unsafe_allow_html=True)
#         
#         # Assign all features with desired data input method
#         sidebar.title('Numerical Features')
#         product_description_lenght = sidebar.slider('product_description_lenght', 4,3990,100)
#         product_photos_qty = sidebar.slider('product_photos_qty', 1,20,1)
#         payment_installments = sidebar.slider('payment_installments', 1,24,1)
#         estimated_days = sidebar.slider('estimated_days', 3,60,1)
#         arrival_days = sidebar.slider('arrival_days', 0,60,1)
#         payment_type = st.selectbox('payment_type', ['credit_card', 'boleto', 'voucher', 'debit_card'])
#         arrival_status = st.selectbox('arrival_status', ['OnTime/Early', 'Late'])
#         seller_to_carrier_status = st.selectbox('seller_to_carrier_status', ['OnTime/Early', 'Late'])
#         estimated_delivery_rate = st.selectbox('estimated_delivery_rate', ['Very Slow', 'Slow', 'Neutral', 'Fast', 'Very Fast'])
#         arrival_delivery_rate = st.selectbox('arrival_delivery_rate', ['Very Slow', 'Slow', 'Neutral', 'Fast', 'Very Fast'])
#         shipping_delivery_rate = st.selectbox('shipping_delivery_rate Date', ['Very Slow', 'Slow', 'Neutral', 'Fast', 'Very Fast'])
#         payment_value = st.text_input('payment_value', '')
#         freight_value = st.text_input('freight_value', '')
#         result = ''
# 
#         # Predict Customer Satsifaction
#         if st.button('Predict_Satisfaction'):
#             result = predict_satisfaction(freight_value, product_description_lenght, product_photos_qty, payment_type, payment_installments, payment_value, 
#                                         estimated_days, arrival_days, arrival_status, seller_to_carrier_status, estimated_delivery_rate, arrival_delivery_rate, shipping_delivery_rate)
#                                         
#         if result == 0:
#             result = 'Not Satisfied'
#             st.success(f'The Customer is {result}')
#         else:
#             result = 'Satisfied'
#             st.success(f'The Customer is {result}')
# 
#     if __name__ == '__main__':
#         main()
# 
# if mode == "Clustering":
# 
#     def predict_clustering(freight_value, price, payment_value, payment_installments, payment_sequential):
# 
#         prediction_clustering = model_clustering.predict(pd.DataFrame({'freight_value' :[freight_value], 'price' :[price], 'payment_installments' :[payment_installments], 'payment_value' :[payment_value], 'payment_sequential' :[payment_sequential]}))
#         return prediction_clustering
# 
#     def main():
# 
#         # Header of Customer Segmentation
#         html_temp="""
#                 <div style="background-color:#F5F5F5">
#                 <h1 style="color:#31333F;text-align:center;"> Customer Segmentation </h1>
#                 </div>
#             """
#         st.markdown(html_temp,unsafe_allow_html=True)
# 
#         # Assign all features with desired data input method
#         payment_installments = st.slider('payment_installments', 1,24,1)
#         payment_sequential = st.slider('payment_sequential', 1,24,1)
#         freight_value = st.text_input('freight_value', '')
#         price = st.text_input('price', '')
#         payment_value = st.text_input('payment_value', '')
#         result_cluster = ''
# 
#         # Predict Cluster of the customer
#         if st.button('Predict_Cluster'):
#             result_cluster = predict_clustering(freight_value, price, payment_value, payment_installments, payment_sequential)
#                                         
#         st.success(f'Customer Cluster is {result_cluster}')
#         
#         # Upload CSV file
#         with st.sidebar.header('Upload your CSV data'):
#             uploaded_file = st.sidebar.file_uploader('Upload your input csv file')
# 
#         if uploaded_file is not None:
# 
#             # Read dataset
#             sample = pd.read_csv(uploaded_file, index_col= 0)
#             
#             # Define sidebar for clustering algorithm
#             selected_algorithm = sidebar.selectbox('Select Clustering Algorithm', ['K-Means', 'Agglomerative'])
# 
#             # Define sidebar for number of clusters
#             selected_clusters = sidebar.slider('Select number of clusters', 2, 10, 1)
# 
#             # Define sidebar for PCA
#             use_pca = sidebar.radio('Use PCA', ['No', 'Yes'])
# 
#             # Drop freight values with zeros
#             sample.drop(sample[sample.freight_value == 0].index, inplace= True)
#             # Reset Index 
#             sample.reset_index(inplace= True, drop= True)
#             # Handle Skeweness in sample data
#             for i in ['freight_value', 'price', 'payment_value', 'payment_installments', 'payment_sequential']:
#                 sample[i] = np.log10(sample[i])
# 
#             # Apply standard scaler
#             sc = StandardScaler(with_mean= False)
#             data_scaled = sc.fit_transform(sample)
# 
#             # Select number of clusters
#             if selected_algorithm == 'Agglomerative':
#                 hc = AgglomerativeClustering(n_clusters= selected_clusters)
#                 y_pred_hc = hc.fit_predict(data_scaled)
# 
#             else:
#                 kmean = KMeans(n_clusters= selected_clusters)
#                 y_pred_kmean = kmean.fit_predict(data_scaled)
# 
#             # Apply PCA
#             pca = PCA(n_components= 2)
#             data_pca = pca.fit_transform(data_scaled)
# 
#             # Select number of clusters for PCA
#             kmean_pca = KMeans(n_clusters= selected_clusters)
#             y_pred_pca = kmean_pca.fit_predict(data_pca)
# 
#             def plot_cluster(data, y_pred, num_clusters):
# 
#                 # Plot Clusters
#                 fig, ax = plt.subplots()
#                 Colors= ['red', 'green', 'blue', 'purple', 'orange', 'royalblue', 'brown', 'grey', 'chocolate', 'fuchsia']
#                 for i in range(num_clusters):
#                     ax.scatter(data[y_pred==i,0], data[y_pred==i,1], c= Colors[i], label= 'Cluster ' + str(i+1))
# 
#                 ax.set_title('Customers Clusters')
#                 ax.legend(loc='upper left', prop={'size':5})
#                 ax.axis('off')
#                 st.pyplot(fig)
# 
#             # Option to select and plot PCA for clustering
#             if use_pca == 'No' and selected_algorithm == 'K-Means':
#                 plot_cluster(data_scaled, y_pred_kmean, selected_clusters)
# 
#             elif use_pca == 'No' and selected_algorithm == 'Agglomerative':
#                 plot_cluster(data_scaled, y_pred_hc, selected_clusters)           
# 
#             else:
#                 plot_cluster(data_pca, y_pred_pca, selected_clusters)    
#         
#         else:
#             st.info('Awaiting for CSV file to be uploaded.')
# 
#     if __name__ == '__main__':
#         main()

! streamlit run Brazilian_Ecommerce_Project.py

"""# 11.0 Wrap up & Conclusion

#### 1- Apply Feature Engineering for some datetime features which are very important in the analysis.
#### 2- Make EDA to gain useful insights like how accurate are estimated delivery dates provided by the website (93.8% Early/OnTime, 6.2% Late).
#### 3- Apply Feature Selection to check best features, where we found that shipping features were important.
#### 4- Apply different ML models to predict customer satisfaction either Satisfied or Non-Satisfied, however the results didn't show good performance on testing specially for minor class Non-Satisfied (67% on testing for Macro avg F1-score).
#### 5- To investigate further why results are not good on testing, we used NLP on customers reviews with Naive bayes model to predict satisfaction but this time based on reviews written by customers.
#### 6- The NLP model have shown much better results than previous models (87% on testing).
#### 7- To understand why results are low on traditional models and high with NLP, we investigated further the Non Satisfied customers reviews and found that more than 90% of words in reviews are related to shipping issues.
#### 8- The conflict we have now is that accuracy of estimated delivery dates by the website only mentions 6.2% of lateness, however non-satisfied customers reviews (which represents 23% of data) tell us that shipping lateness is the main reason for non-satisfaction.
#### 9- From this conflict we conclude that the accuracy of estimated delivery dates by the website is misleading because it's only from website perspective or carrier perspective not the customer, for ex maybe the customer didn't receive the order but on the system it shows that it was delivered, or maybe it was delivered late but on the system doesn't show it was delivered late and that's because the customer has no contribution to the system except for written reviews.
#### 10- As a conclusion if we need to predict customer satisfactio based on transaction features we should handle the previous issue first, or the model will always provide bad results to predict satisfaction because of the overlaping between satisfied and non-satisfied which is caused due to the issue mentioned.
#### 11- After that we make customer segmentation using RFM analysis and K-Means, however RFM analysis provides better segmentation as K-Means has high level of overlaping between segments.
#### 12- Finally we deploy Streamlit web App for EDA, Classification and Clustering providing the option to upload sample data and investigate it by the App.
"""